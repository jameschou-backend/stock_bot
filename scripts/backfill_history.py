#!/usr/bin/env python3
"""æ­·å²è³‡æ–™å›è£œè…³æœ¬

ç”¨æ–¼æŠ“å–éå» N å¹´çš„æ­·å²è³‡æ–™ï¼Œæ”¯æ´ï¼š
1. ä¸­æ–·çºŒå‚³ï¼ˆå¾ä¸Šæ¬¡é€²åº¦ç¹¼çºŒï¼‰
2. é€²åº¦é¡¯ç¤ºèˆ‡é ä¼°æ™‚é–“
3. Rate Limitingï¼ˆé¿å…è§¸ç™¼ API é™æµï¼‰
4. æ‰¹æ¬¡æŸ¥è©¢å„ªåŒ–ï¼ˆå¤§å¹…æ¸›å°‘ API æ¬¡æ•¸ï¼‰

ä½¿ç”¨æ–¹å¼ï¼š
    # æŠ“å–éå» 5 å¹´è³‡æ–™
    python scripts/backfill_history.py --years 5
    
    # æŠ“å–ç‰¹å®šæ—¥æœŸç¯„åœ
    python scripts/backfill_history.py --start 2021-01-01 --end 2026-02-05
    
    # é¡¯ç¤ºç›®å‰é€²åº¦å’Œé ä¼°
    python scripts/backfill_history.py --status
    
    # é‡ç½®é€²åº¦ï¼ˆé‡æ–°æŠ“å–ï¼‰
    python scripts/backfill_history.py --reset --years 5

API ç”¨é‡ä¼°ç®—ï¼ˆä»˜è²»æœƒå“¡ 6000 æ¬¡/å°æ™‚ï¼‰ï¼š
- å…¨å¸‚å ´æ¨¡å¼ï¼š5 å¹´è³‡æ–™ç´„éœ€ 20-40 æ¬¡ API calls
- é€æª”æ‰¹æ¬¡æ¨¡å¼ï¼š5 å¹´è³‡æ–™ç´„éœ€ 360-720 æ¬¡ API calls
"""

from __future__ import annotations

import argparse
import json
import sys
import time
from dataclasses import dataclass
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from zoneinfo import ZoneInfo

# åŠ å…¥ project root åˆ° path
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

import pandas as pd
from sqlalchemy import func
from sqlalchemy.dialects.mysql import insert

from app.config import load_config
from app.db import get_session
from app.finmind import (
    DEFAULT_CHUNK_DAYS,
    FinMindError,
    date_chunks,
    fetch_dataset,
    fetch_dataset_by_stocks,
    fetch_stock_list,
)
from app.models import RawInstitutional, RawPrice
from app.rate_limiter import get_rate_limiter
from skills.ingest_institutional import _normalize_institutional
from skills.ingest_prices import _normalize_prices


# é€²åº¦æª”æ¡ˆä½ç½®
PROGRESS_FILE = Path(__file__).resolve().parent.parent / "storage" / "backfill_progress.json"

PRICE_DATASET = "TaiwanStockPrice"
INST_DATASET = "TaiwanStockInstitutionalInvestorsBuySell"


@dataclass
class BackfillProgress:
    """å›è£œé€²åº¦è¿½è¹¤"""
    start_date: date
    end_date: date
    current_date: date  # ç›®å‰è™•ç†åˆ°çš„æ—¥æœŸ
    total_chunks: int
    completed_chunks: int
    prices_rows: int
    inst_rows: int
    api_calls: int
    fetch_mode: str  # "bulk" or "by_stock"
    started_at: str
    last_updated: str
    
    def to_dict(self) -> Dict:
        return {
            "start_date": self.start_date.isoformat(),
            "end_date": self.end_date.isoformat(),
            "current_date": self.current_date.isoformat(),
            "total_chunks": self.total_chunks,
            "completed_chunks": self.completed_chunks,
            "prices_rows": self.prices_rows,
            "inst_rows": self.inst_rows,
            "api_calls": self.api_calls,
            "fetch_mode": self.fetch_mode,
            "started_at": self.started_at,
            "last_updated": self.last_updated,
        }
    
    @classmethod
    def from_dict(cls, d: Dict) -> "BackfillProgress":
        return cls(
            start_date=date.fromisoformat(d["start_date"]),
            end_date=date.fromisoformat(d["end_date"]),
            current_date=date.fromisoformat(d["current_date"]),
            total_chunks=d["total_chunks"],
            completed_chunks=d["completed_chunks"],
            prices_rows=d["prices_rows"],
            inst_rows=d["inst_rows"],
            api_calls=d["api_calls"],
            fetch_mode=d["fetch_mode"],
            started_at=d["started_at"],
            last_updated=d["last_updated"],
        )


def load_progress() -> Optional[BackfillProgress]:
    """è¼‰å…¥é€²åº¦æª”æ¡ˆ"""
    if not PROGRESS_FILE.exists():
        return None
    try:
        with open(PROGRESS_FILE, "r") as f:
            return BackfillProgress.from_dict(json.load(f))
    except Exception:
        return None


def save_progress(progress: BackfillProgress) -> None:
    """å„²å­˜é€²åº¦æª”æ¡ˆ"""
    PROGRESS_FILE.parent.mkdir(parents=True, exist_ok=True)
    progress.last_updated = datetime.now().isoformat()
    with open(PROGRESS_FILE, "w") as f:
        json.dump(progress.to_dict(), f, indent=2)


def delete_progress() -> None:
    """åˆªé™¤é€²åº¦æª”æ¡ˆ"""
    if PROGRESS_FILE.exists():
        PROGRESS_FILE.unlink()


def estimate_api_calls(
    start_date: date,
    end_date: date,
    chunk_days: int,
    fetch_mode: str,
    stock_count: int = 1800,
    batch_size: int = 100,
) -> Tuple[int, int]:
    """ä¼°ç®— API æ¬¡æ•¸å’Œæ‰€éœ€æ™‚é–“
    
    Returns:
        (estimated_calls, estimated_minutes)
    """
    total_days = (end_date - start_date).days
    chunks = (total_days + chunk_days - 1) // chunk_days
    
    if fetch_mode == "bulk":
        # å…¨å¸‚å ´æ¨¡å¼ï¼šæ¯å€‹ chunk æŠ“ 2 æ¬¡ï¼ˆåƒ¹æ ¼+æ³•äººï¼‰+ 1 æ¬¡ stock list
        calls = chunks * 2 + 1
    else:
        # é€æª”æ‰¹æ¬¡æ¨¡å¼ï¼šæ¯å€‹ chunk æŠ“ (stock_count / batch_size) * 2 æ¬¡
        batches_per_chunk = (stock_count + batch_size - 1) // batch_size
        calls = chunks * batches_per_chunk * 2 + 1
    
    # ä»¥æ¯å°æ™‚ 5400 æ¬¡ï¼ˆ90% bufferï¼‰è¨ˆç®—æ‰€éœ€æ™‚é–“
    minutes = calls / 90  # 5400 / 60 = 90 calls per minute
    
    return calls, int(minutes) + 1


def print_status(progress: Optional[BackfillProgress], config) -> None:
    """å°å‡ºç›®å‰ç‹€æ…‹"""
    print("\n" + "=" * 60)
    print("ğŸ“Š æ­·å²è³‡æ–™å›è£œç‹€æ…‹")
    print("=" * 60)
    
    if progress is None:
        print("âŒ æ²’æœ‰é€²è¡Œä¸­çš„å›è£œä»»å‹™")
        return
    
    pct = (progress.completed_chunks / progress.total_chunks * 100) if progress.total_chunks > 0 else 0
    remaining_chunks = progress.total_chunks - progress.completed_chunks
    
    # ä¼°ç®—å‰©é¤˜æ™‚é–“
    est_calls, est_minutes = estimate_api_calls(
        progress.current_date,
        progress.end_date,
        config.chunk_days,
        progress.fetch_mode,
    )
    
    print(f"ğŸ“… æ—¥æœŸç¯„åœ: {progress.start_date} ~ {progress.end_date}")
    print(f"ğŸ“ ç›®å‰é€²åº¦: {progress.current_date}")
    print(f"ğŸ“¦ Chunks:   {progress.completed_chunks}/{progress.total_chunks} ({pct:.1f}%)")
    print(f"ğŸ”„ æŠ“å–æ¨¡å¼: {progress.fetch_mode}")
    print(f"ğŸ“ˆ å·²æŠ“å–:   prices={progress.prices_rows:,} rows, institutional={progress.inst_rows:,} rows")
    print(f"ğŸ”— API å‘¼å«: {progress.api_calls:,} æ¬¡")
    print(f"â±ï¸  é ä¼°å‰©é¤˜: ~{est_calls:,} æ¬¡ API calls, ~{est_minutes} åˆ†é˜")
    print(f"ğŸ• é–‹å§‹æ™‚é–“: {progress.started_at}")
    print(f"ğŸ• æ›´æ–°æ™‚é–“: {progress.last_updated}")
    
    # Rate limiter ç‹€æ…‹
    limiter = get_rate_limiter(config.finmind_requests_per_hour)
    remaining = limiter.remaining_requests()
    print(f"âš¡ Rate Limit: {remaining}/{limiter.effective_limit} å‰©é¤˜ï¼ˆæœ¬å°æ™‚ï¼‰")
    print("=" * 60)


def run_backfill(
    start_date: date,
    end_date: date,
    config,
    resume: bool = True,
) -> Dict:
    """åŸ·è¡Œæ­·å²è³‡æ–™å›è£œ
    
    Args:
        start_date: é–‹å§‹æ—¥æœŸ
        end_date: çµæŸæ—¥æœŸ
        config: AppConfig
        resume: æ˜¯å¦å¾ä¸Šæ¬¡é€²åº¦ç¹¼çºŒ
    
    Returns:
        çµæœæ‘˜è¦ dict
    """
    # è¼‰å…¥æˆ–å»ºç«‹é€²åº¦
    progress = load_progress() if resume else None
    
    if progress and (progress.start_date != start_date or progress.end_date != end_date):
        print(f"âš ï¸  é€²åº¦æª”æ¡ˆçš„æ—¥æœŸç¯„åœä¸ç¬¦ï¼Œå°‡é‡æ–°é–‹å§‹")
        progress = None
    
    # è¨ˆç®— chunks
    chunk_days = config.chunk_days
    all_chunks = list(date_chunks(start_date, end_date, chunk_days))
    total_chunks = len(all_chunks)
    
    # æ±ºå®šèµ·å§‹ chunk
    start_chunk_idx = 0
    if progress:
        # æ‰¾åˆ°æ‡‰è©²ç¹¼çºŒçš„ chunk
        for idx, (chunk_start, _) in enumerate(all_chunks):
            if chunk_start >= progress.current_date:
                start_chunk_idx = idx
                break
        print(f"ğŸ“‚ å¾ chunk {start_chunk_idx + 1}/{total_chunks} ç¹¼çºŒ")
    else:
        progress = BackfillProgress(
            start_date=start_date,
            end_date=end_date,
            current_date=start_date,
            total_chunks=total_chunks,
            completed_chunks=0,
            prices_rows=0,
            inst_rows=0,
            api_calls=0,
            fetch_mode="bulk",
            started_at=datetime.now().isoformat(),
            last_updated=datetime.now().isoformat(),
        )
    
    # åˆå§‹åŒ– Rate Limiter
    limiter = get_rate_limiter(config.finmind_requests_per_hour)
    
    # å…ˆå˜—è©¦å…¨å¸‚å ´æŠ“å–ï¼Œå¤±æ•—å‰‡ç²å–è‚¡ç¥¨æ¸…å–®
    fetch_mode = "bulk"
    stock_list: List[str] = []
    
    print(f"\nğŸš€ é–‹å§‹å›è£œæ­·å²è³‡æ–™")
    print(f"ğŸ“… æ—¥æœŸç¯„åœ: {start_date} ~ {end_date}")
    print(f"ğŸ“¦ ç¸½ chunks: {total_chunks}, chunk_days: {chunk_days}")
    
    # ä¼°ç®—
    est_calls_bulk, est_minutes_bulk = estimate_api_calls(start_date, end_date, chunk_days, "bulk")
    est_calls_stock, est_minutes_stock = estimate_api_calls(start_date, end_date, chunk_days, "by_stock")
    print(f"ğŸ“Š é ä¼° API æ¬¡æ•¸:")
    print(f"   - å…¨å¸‚å ´æ¨¡å¼: ~{est_calls_bulk} æ¬¡ (~{est_minutes_bulk} åˆ†é˜)")
    print(f"   - é€æª”æ‰¹æ¬¡æ¨¡å¼: ~{est_calls_stock} æ¬¡ (~{est_minutes_stock} åˆ†é˜)")
    print()
    
    with get_session() as db_session:
        for chunk_idx, (chunk_start, chunk_end) in enumerate(all_chunks[start_chunk_idx:], start=start_chunk_idx):
            chunk_num = chunk_idx + 1
            
            # é€²åº¦é¡¯ç¤º
            pct = chunk_num / total_chunks * 100
            remaining = limiter.remaining_requests()
            print(f"[{chunk_num}/{total_chunks}] ({pct:.1f}%) {chunk_start} ~ {chunk_end} | Rate: {remaining}/{limiter.effective_limit}", end="", flush=True)
            
            # === æŠ“å–åƒ¹æ ¼è³‡æ–™ ===
            price_df = pd.DataFrame()
            try:
                price_df = fetch_dataset(
                    PRICE_DATASET,
                    chunk_start,
                    chunk_end,
                    token=config.finmind_token,
                    requests_per_hour=config.finmind_requests_per_hour,
                )
                progress.api_calls += 1
                
                # å¦‚æœå…¨å¸‚å ´æŠ“å–å›å‚³ç©ºå€¼ï¼Œåˆ‡æ›åˆ°é€æª”æ¨¡å¼
                if price_df.empty and fetch_mode == "bulk":
                    print(" [åˆ‡æ›åˆ°é€æª”æ¨¡å¼]", end="", flush=True)
                    fetch_mode = "by_stock"
                    progress.fetch_mode = "by_stock"
                    
                    # å–å¾—è‚¡ç¥¨æ¸…å–®
                    stock_list = fetch_stock_list(config.finmind_token, config.finmind_requests_per_hour)
                    progress.api_calls += 1
                    print(f" [è‚¡ç¥¨æ•¸: {len(stock_list)}]", end="", flush=True)
                
                if price_df.empty and fetch_mode == "by_stock" and stock_list:
                    price_df = fetch_dataset_by_stocks(
                        PRICE_DATASET,
                        chunk_start,
                        chunk_end,
                        stock_list,
                        token=config.finmind_token,
                        requests_per_hour=config.finmind_requests_per_hour,
                    )
                    # ä¼°ç®— API æ¬¡æ•¸ï¼ˆæ‰¹æ¬¡æŸ¥è©¢ï¼‰
                    progress.api_calls += (len(stock_list) + 99) // 100
                
            except FinMindError as e:
                print(f" âŒ prices: {e}")
            
            # å¯«å…¥åƒ¹æ ¼è³‡æ–™
            if not price_df.empty:
                try:
                    price_df = _normalize_prices(price_df)
                    records = price_df.to_dict("records")
                    if records:
                        stmt = insert(RawPrice).values(records)
                        update_cols = {col: stmt.inserted[col] for col in ["open", "high", "low", "close", "volume"]}
                        stmt = stmt.on_duplicate_key_update(**update_cols)
                        db_session.execute(stmt)
                        db_session.commit()
                        progress.prices_rows += len(records)
                except Exception as e:
                    print(f" âš ï¸ prices write: {e}")
            
            # === æŠ“å–æ³•äººè³‡æ–™ ===
            inst_df = pd.DataFrame()
            try:
                inst_df = fetch_dataset(
                    INST_DATASET,
                    chunk_start,
                    chunk_end,
                    token=config.finmind_token,
                    requests_per_hour=config.finmind_requests_per_hour,
                )
                progress.api_calls += 1
                
                if inst_df.empty and fetch_mode == "by_stock" and stock_list:
                    inst_df = fetch_dataset_by_stocks(
                        INST_DATASET,
                        chunk_start,
                        chunk_end,
                        stock_list,
                        token=config.finmind_token,
                        requests_per_hour=config.finmind_requests_per_hour,
                    )
                    progress.api_calls += (len(stock_list) + 99) // 100
                    
            except FinMindError as e:
                print(f" âŒ inst: {e}")
            
            # å¯«å…¥æ³•äººè³‡æ–™
            if not inst_df.empty:
                try:
                    inst_df = _normalize_institutional(inst_df)
                    records = inst_df.to_dict("records")
                    if records:
                        stmt = insert(RawInstitutional).values(records)
                        update_cols = {
                            col: stmt.inserted[col]
                            for col in [
                                "foreign_buy", "foreign_sell", "foreign_net",
                                "trust_buy", "trust_sell", "trust_net",
                                "dealer_buy", "dealer_sell", "dealer_net",
                            ]
                        }
                        stmt = stmt.on_duplicate_key_update(**update_cols)
                        db_session.execute(stmt)
                        db_session.commit()
                        progress.inst_rows += len(records)
                except Exception as e:
                    print(f" âš ï¸ inst write: {e}")
            
            # æ›´æ–°é€²åº¦
            progress.completed_chunks = chunk_num
            progress.current_date = chunk_end + timedelta(days=1)
            save_progress(progress)
            
            print(f" âœ… prices: +{len(price_df) if not price_df.empty else 0}, inst: +{len(inst_df) if not inst_df.empty else 0}")
    
    # å®Œæˆ
    print(f"\nğŸ‰ å›è£œå®Œæˆ!")
    print(f"ğŸ“ˆ ç¸½è¨ˆ: prices={progress.prices_rows:,} rows, institutional={progress.inst_rows:,} rows")
    print(f"ğŸ”— ç¸½ API å‘¼å«: {progress.api_calls:,} æ¬¡")
    
    # åˆªé™¤é€²åº¦æª”æ¡ˆï¼ˆå®Œæˆå¾Œï¼‰
    delete_progress()
    
    return {
        "prices_rows": progress.prices_rows,
        "inst_rows": progress.inst_rows,
        "api_calls": progress.api_calls,
        "fetch_mode": fetch_mode,
    }


def main():
    parser = argparse.ArgumentParser(
        description="æ­·å²è³‡æ–™å›è£œè…³æœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )
    parser.add_argument("--years", type=int, default=5, help="æŠ“å–éå»å¹¾å¹´è³‡æ–™ (é è¨­: 5)")
    parser.add_argument("--start", type=str, help="é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--end", type=str, help="çµæŸæ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--status", action="store_true", help="é¡¯ç¤ºç›®å‰é€²åº¦")
    parser.add_argument("--reset", action="store_true", help="é‡ç½®é€²åº¦")
    parser.add_argument("--estimate", action="store_true", help="åªé¡¯ç¤º API ç”¨é‡ä¼°ç®—")
    
    args = parser.parse_args()
    
    config = load_config()
    
    # é¡¯ç¤ºç‹€æ…‹
    if args.status:
        progress = load_progress()
        print_status(progress, config)
        return
    
    # é‡ç½®é€²åº¦
    if args.reset:
        delete_progress()
        print("âœ… é€²åº¦å·²é‡ç½®")
        if not args.years and not args.start:
            return
    
    # æ±ºå®šæ—¥æœŸç¯„åœ
    today = datetime.now(ZoneInfo(config.tz)).date()
    
    if args.start and args.end:
        start_date = date.fromisoformat(args.start)
        end_date = date.fromisoformat(args.end)
    else:
        end_date = today
        start_date = today - timedelta(days=365 * args.years)
    
    # åªé¡¯ç¤ºä¼°ç®—
    if args.estimate:
        print(f"\nğŸ“Š API ç”¨é‡ä¼°ç®— ({start_date} ~ {end_date})")
        print(f"   chunk_days: {config.chunk_days}")
        
        est_bulk, min_bulk = estimate_api_calls(start_date, end_date, config.chunk_days, "bulk")
        est_stock, min_stock = estimate_api_calls(start_date, end_date, config.chunk_days, "by_stock")
        
        print(f"\n   å…¨å¸‚å ´æ¨¡å¼:")
        print(f"     - API æ¬¡æ•¸: ~{est_bulk} æ¬¡")
        print(f"     - é ä¼°æ™‚é–“: ~{min_bulk} åˆ†é˜")
        
        print(f"\n   é€æª”æ‰¹æ¬¡æ¨¡å¼:")
        print(f"     - API æ¬¡æ•¸: ~{est_stock} æ¬¡")
        print(f"     - é ä¼°æ™‚é–“: ~{min_stock} åˆ†é˜")
        
        print(f"\n   âš¡ æ¯å°æ™‚é™åˆ¶: {config.finmind_requests_per_hour} æ¬¡")
        return
    
    # æª¢æŸ¥ token
    if not config.finmind_token:
        print("âŒ éŒ¯èª¤: FINMIND_TOKEN æœªè¨­å®š")
        print("è«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®š FINMIND_TOKEN")
        sys.exit(1)
    
    # åŸ·è¡Œå›è£œ
    result = run_backfill(start_date, end_date, config, resume=not args.reset)
    print(f"\nçµæœ: {result}")


if __name__ == "__main__":
    main()
